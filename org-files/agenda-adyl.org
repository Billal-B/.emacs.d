#+TITLE: agenda-adyl
#+TODO: TODO IN-PROGRESS | DONE
#+COLUMNS: %25ITEM %5TODO %1PRIORITY %10TAGS
#+COLUMNS: %25ITEM(Task) %5TODO(To-do) %1PRIORITY %10TAGS
* TODOS
:CATEGORY: ADYL
** DONE AYLDATA-370 Export billing stat by hour
  CLOSED: [2019-12-03 mar. 17:40]

** DONE IN-PROGRESS [#A] manadge deal stat
   CLOSED: [2019-12-16 lun. 17:04]
  :LOGBOOK:
  CLOCK: [2019-12-03 Thu 16:34]
  :END:
- [ ] Make in work in cluster and check the output in GCS
- [ ] Clean up the code

** DONE [#A] Fix manadge job
   CLOSED: [2019-12-16 lun. 17:04]
  :LOGBOOK:
  CLOCK: [2019-13-04 mer. 11:54]--[2019-12-04 mer. 11:54] =>  0:00
  :END:
- [X] AYLDATA-371 DealType should be RevenueAuctionType from partner table in Manadge Deal export
   CLOSED: [2019-12-03 mar. 17:40]
- [X] AYLDATA-373 Export Deals in json format with lists column
   CLOSED: [2019-12-03 mar. 17:40]
- [X] AYLDATA-374 BuyerId should be SeatId
- [X] advertiser (todo in deal id pr)
- [X] TODO AYLDATA-375 FloorPrice should be in 10e-3$ not 10e-6
** DONE [#C] investigate why some days are skipped in job
- [ ] ./scripts/run_job.sh -s user -j TransformBillingStats -a "from=2019-12-01 to=2019-12-02"
- [ ] ./scripts/run_job.sh -s user -j manadge.ExportDeal -a "from=2019-12-01 to=2019-12-02"

** DONE [#A] ask gregoire if we want to load all the platform DS (ie a month) by not specifying from to dates
  CLOSED: [2019-12-12 jeu. 14:13] DEADLINE: <2019-12-12 jeu.>
because it makes the join 30 times bigger

** DONE [#A] slack notif
   CLOSED: [2019-12-24 mar. 10:52]
- to be done in airflow python code
- there is an airflow connector ?
https://medium.com/datareply/integrating-slack-alerts-in-airflow-c9dcd155105
- slack creds
cluster leak checker

** DONE [#A] update airflow version
   CLOSED: [2019-12-24 mar. 10:52]
bc of bug https://issues.apache.org/jira/browse/AIRFLOW-4262 which impact slack webhook
** DONE [#B] Aggregate AdServerStat by attempt
   CLOSED: [2020-01-29 mer. 15:33] DEADLINE: <2020-01-15 mer.>
   :LOGBOOK:
   CLOCK: [2019-12-24 mar. 10:53]
   :END:

** DONE [#A] tell greg that there are multiple visitor per attempt
   CLOSED: [2020-01-02 jeu. 15:37]
   [2019-12-31 mar.]
** DONE [#A] no deal id in stat + broken revenue for deal stat manadge
   CLOSED: [2020-01-06 lun. 17:02]
   [2020-01-03 ven. 14:56]
** DONE [#A] ask chris or JM why no deal id in ad server stat
   CLOSED: [2020-01-06 lun. 11:37]
   [2020-01-03 ven. 17:08]
** DONE ask chris if it's useful to have adserver stats for adserver grouped by attempt but without dealID
   CLOSED: [2020-01-06 lun. 11:36]
   [2020-01-06 lun. 10:56]
** DONE [#A] manadge : put stat with deal id in different file than stat without
   CLOSED: [2020-01-08 mer. 09:50]
   [2020-01-06 lun. 16:59]
** DONE [#A] deploy airflow on prod
   CLOSED: [2020-01-08 mer. 09:50]
   [2020-01-07 mar. 10:32]
** DONE [#B] add min item/max item/min user/max user in als hyper opt
   DEADLINE: <2020-01-09 jeu.>
hyper opt res are written in conf
   [2020-01-07 mar. 10:55]
** DONE [#A] investigate why some days are skipped in billing job
   DEADLINE: <2020-01-08 mer.>
   [2020-01-08 mer. 10:28]
- [ ] no billing data in postgre for 2019-12-07
- [ ] no postgre-sql backup for the 2019-12-06 (company, realm, placement, site, broker_partner)
*** issue is in adserverStat data loading
turns out it was because of :
 ~if (from.until(LocalDate.now()).getDays > 5) coldLocation else location~
- in transformBillingStat : dataSource.count = 0 & companyContext (etc...) !=0
**** ./scripts/run_job.sh -s user -j TransformBillingStats -a "from=2020-01-02 to=2020-01-03"
>>>>>>>> parquetLocation CloudDirLocation(ayl-datawarehouse-adserver-stats-cold,)
>>>>>>>> base pathgs://ayl-datawarehouse-adserver-stats-cold
>>>>>>>> from2020-01-02
>>>>>>>> to2020-01-03
**** ./scripts/run_job.sh -s user -j TransformBillingStats -a "from=2019-12-07 to=2019-12-08"
>>>>>>>> parquetLocation CloudDirLocation(ayl-datawarehouse-adserver-stats,)
>>>>>>>> base pathgs://ayl-datawarehouse-adserver-stats
>>>>>>>> from2019-12-07
>>>>>>>> to2019-12-08
** DONE review Alexis PR on scala 2.12
SCHEDULED: <2020-01-14 mar.>
:PROPERTIES:
:ID:       a4eefc55-9ca4-45e8-b9e2-df225a6a3ee4
:END:
  [2020-01-13 lun. 15:50]

** DONE [Billing] fix stats
CLOSED: [2020-01-17 ven. 14:26] SCHEDULED: <2020-01-14 mar.>
*** The stats are correct on a day basis but are broken on a hour
*** Query : number of timestamp where hour = 00
df.withColumn("timestamp", timestamp($"PublishedAt")).select($"publishedat", $"timestamp").where($"timestamp" === "2020-01-10 00:00:00").count
res2: Long = 98681727

scala> val df = spark.read.parquet("gs://ayl-datawarehouse-adserver-stats/day=2020-01-10/hour=00").cache
df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Uid: string, DC: string ... 48 more fields]

scala> df.withColumn("timestamp", timestamp($"PublishedAt")).select($"publishedat", $"timestamp").where($"timestamp" === "2020-01-10 00:00:00").count
res9: Long = 98681727

scala> df.count()
res10: Long = 98681727

=> the issue is not with the timestamp fn
*** The stats are correct on a day basis but are broken on a hour basis
*** Big offset for the hour 00 and small offset for other hours

*** If timestamp for price pub & price adv then take price pub TS
*** Don't groupBy attempt
** DONE find why manadge job fails on the 5th november
   CLOSED: [2020-01-21 mar. 10:10]
   [2020-01-15 mer. 11:18]
   It was because we tried to load the data with the wrong schema.
   Solution : explicitely specify the

** DONE create ETL job clickers identification
   CLOSED: [2020-02-05 mer. 14:35]
   [2020-01-21 mar. 12:05]
Create a job that load data using date-etl-job context. Need to create a case class that extends a base trait
for adServerStat, and add annotation in the case class def.
Only keep adserver where eventKind is visible impression or click
Name it VisibleImpression.
*** data are 1.5GB, too much ?
val df = spark.read.parquet("gs://ayl-datawarehouse-adserver-stats/day=2020-01-21/hour=00").cache
df.groupBy($"Attempt").agg(first($"DC")).count()
res2: Long = 36760831
df.where($"EventKind" === "CLICK" || $"EventKind" === "VISIBLE_IMPRESSION").groupBy($"Attempt").agg(first($"DC")).count()
res3: Long = 417580
** DONE set date to the 26 in data-etl-job
   CLOSED: [2020-02-03 lun. 11:00]
   [2020-01-28 mar. 17:14]
** DONE filter on origin for clickers etl
   CLOSED: [2020-02-03 lun. 11:00]
   [2020-01-29 mer. 15:15]
** DONE check if the transform adserver stat job works correctly
   CLOSED: [2020-02-03 lun. 11:00]
   [2020-01-29 mer. 15:52]
** DONE reads greg PR on manadge stat
   CLOSED: [2020-02-11 mar. 09:41]
   [2020-02-05 mer. 17:18]
** DONE Make a DagBuilder for data-scala-ml
   CLOSED: [2020-02-26 mer. 17:31]
To run make airflow-dev
   [2020-02-10 lun. 18:46]
** DONE Refactor dagBuilder (data-scala-ml) + extract timestamp from date (visibleImpression)
   CLOSED: [2020-02-26 mer. 17:32]
   [2020-02-11 mar. 17:32]
** DONE Check rtb transactions
   CLOSED: [2020-02-26 mer. 17:31]
   [2020-02-26 mer. 11:43]
** DONE review alexis PR MLeap model actor serving
   CLOSED: [2020-03-25 mer. 15:31]
   [2020-03-03 mar. 18:07]
** DONE adds run in currencies makefile and use it in data-job-scheduler
   CLOSED: [2020-03-25 mer. 15:31]
   [2020-03-23 lun. 11:35]

** Currencies
*** DONE new dag data job scheduler
    CLOSED: [2020-03-25 mer. 15:32]
    [2020-03-25 mer. 15:29]
*** DONE deploy post gres in kube
    CLOSED: [2020-03-25 mer. 16:48]
    [2020-03-25 mer. 15:29]
*** DONE restore prod data on db
    CLOSED: [2020-03-27 ven. 16:33]
    [2020-03-25 mer. 15:30]
*** IN-PROGRESS schedule job daily
    [2020-03-25 mer. 15:30]
* Agenda
** 2019
*** 2019-10 octobre
**** 07/10
***** worked on bandpass filter test
  ie filter rare and too frequent users
  - worked on the test case
  - worked on integrating bandpass filter
  filtering takes around 4500ms
**** 09/10
***** Successfully loaded tf model in Java ?
  - Need to test if its working bc the input/output layers given by Alexis throw [IllegalArgumentException: No Operation named [serving_dense_3] in the Graph]
***** Need to write python script to draw graph from saved model
**** 10/10
***** changed some tests in data-common

**** 11/10
***** successfully loaded the tf model and got the result from the ouput layer
**** 14/10
***** DONE need to batch predictions
      CLOSED: [2019-10-14 lun. 16:01]
***** DONE need to wrap tf in an akka actor
      CLOSED: [2019-10-14 lun. 16:01]
***** DONE set up akka cluster
      CLOSED: [2019-10-14 lun. 16:24]
**** 17/10
***** got back on bandpass filtering
  flatmapgroups(2000k) in ms
  - 7000
  groupBy time (200k) in ms
  - 11014 Some(JOIN TIME 4981)
  - reuse group DS : 16000 / 4000
  - reuse group DS + persist them : 20000 / 5500
  - reuse group DS + cache them : 18543 JOIN TIME 4914

**** 21/10
***** DONE make test for ad transform job
      CLOSED: [2019-10-22 mar. 11:53]
***** DONE make null value for deprecated field in secor
      CLOSED: [2019-10-22 mar. 11:53]
**** 22/10
***** DONE make test for FileReader / DatedFileReader
      CLOSED: [2019-10-23 mer. 09:34]
**** 23/10
***** DONE use spark shell to write secor data in parquet
      CLOSED: [2019-10-23 mer. 17:55]
  then compare with json to find if its faster
**** 24/10
**** 25/10
***** DONE find why spark cant explode with 11.4.1 snapshot
      CLOSED: [2020-01-08 mer. 10:28]
*** 2019-11 novembre
**** 05/11
***** DONE remove cached file manager / azure FM / listRecursive + clean unused methods
      CLOSED: [2019-11-14 jeu. 11:03]
**** 14/11
***** created class SqlWriter because we don't need a cloudFileManager that needs to be provided with FileWriter
** 2020
*** 2020-01 janvier
**** 2020-01-02 jeudi
***** fixed a broken regexp that broke in 2020
dateColumn=(201[0-9]-[0-9]*-[0-9]*) => dateColumn=(2[0-9]{3}-[0-9]*-[0-9]*)
    Entered on [2020-01-02 jeu. 15:44]
*** 2020-02 f√©vrier
**** 2020-02-03 lundi
***** finished dockerfile
    Entered on [2020-02-03 lun. 18:44]
***** got jenkins almost to work
    Entered on [2020-02-03 lun. 18:45]
***** updated my branch on clickers serving with the one of Alexis
    Entered on [2020-02-03 lun. 18:45]
**** 2020-02-04 mardi
***** DONE done company export job
    Entered on [2020-02-04 mar. 12:07]
***** DONE fixes permission issue with postgre
    Entered on [2020-02-04 mar. 16:18]
***** DONE merged alexis PR but need to fix test issue
      CLOSED: [2020-02-05 mer. 14:35]
AYL common version was update but now there is an issue :
Symbol 'type org.scalatest.funsuite.AnyFunSuiteLike' is missing from the classpath
    Entered on [2020-02-04 mar. 18:03]
**** 2020-02-05 mercredi
***** fixed jenkinsfile for clickers serving
    Entered on [2020-02-05 mer. 14:31]
***** Small fix on manadge deal to set the day
Also had to fix a dependency problem
    Entered on [2020-02-05 mer. 16:13]
**** 2020-02-06 jeudi
***** updated bandpassfilter branch
had to do a complex rebase
    Entered on [2020-02-06 jeu. 16:47]
***** reviewed code for logReg job
    Entered on [2020-02-06 jeu. 17:10]
**** 2020-02-07 vendredi
***** wrote conf for clickers log reg job
    Entered on [2020-02-07 ven. 13:59]
***** face issue deduplicate: different file contents found in the following
    Entered on [2020-02-07 ven. 15:31]
**** 2020-02-10 lundi
***** fixes spark deps issues for log reg job
it was caused by mleap which comes with spark 2.4.0
    Entered on [2020-02-10 lun. 11:25]
***** took me all day to fix a deps issue (a spark class was not present in the jar)
    Entered on [2020-02-10 lun. 18:23]
***** Finished jenkinsfile ? needs to test on prod/dev env
    Entered on [2020-02-10 lun. 18:31]
**** 2020-02-11 mardi
***** DONE Needs to add a timestamp column in Visibleimpression
      CLOSED: [2020-03-27 ven. 16:34]
    Entered on [2020-02-11 mar. 17:00]
***** finished working on airflow conf for clickers
***** lost time because i used the wrong cluster conf
***** DONE see alexis comment on https://github.com/Adyoulike/data-etl-jobs/pull/51#discussion_r377763869
      CLOSED: [2020-03-27 ven. 16:34]
related to timestamp in milli
**** 2020-02-17 lundi
***** Enriched company table with Country Name + Realm Publisher
    Entered on [2020-02-17 lun. 11:56]
***** fix for daily job and postgresql
    Entered on [2020-02-17 lun. 16:54]
**** 2020-02-18 mardi
***** done makes dagbuilder generic (data-common)
    Entered on [2020-02-18 mar. 14:45]
***** selectAs type check
    Entered on [2020-02-18 mar. 14:56]
**** 2020-02-19 mercredi
***** fixed incorrect output schema for data-etl-jobs
    Entered on [2020-02-19 mer. 14:04]
***** added ccpa metrics
    Entered on [2020-02-19 mer. 17:09]
***** finished generic dag builder (etl)
    Entered on [2020-02-19 mer. 17:09]
**** 2020-02-20 jeudi
***** Done realmId and currency in company table PR
    Entered on [2020-02-20 jeu. 13:45]
***** began working on curiosity project
    Entered on [2020-02-20 jeu. 13:45]
**** 2020-02-21 vendredi
***** Tested CCPA (it works)
    Entered on [2020-02-21 ven. 16:54]
***** worked on curiosity setup (jenkins) with greg
    Entered on [2020-02-21 ven. 16:54]
***** started work on auto doc
    Entered on [2020-02-21 ven. 16:55]
**** 2020-02-24 lundi
***** worked on auto doc
    Entered on [2020-02-24 lun. 14:39]
***** check ccpa metrics
    Entered on [2020-02-24 lun. 18:13]
**** 2020-02-25 mardi
***** finished ccpa
    Entered on [2020-02-25 mar. 11:26]
***** created rtb transaction airflow job
    Entered on [2020-02-25 mar. 11:26]
***** updated data-etl-job ver in datawarehouse
    Entered on [2020-02-25 mar. 18:29]
**** 2020-02-26 mercredi
***** worked on curiosity poetry jenkins integration
    Entered on [2020-02-26 mer. 12:10]
***** continues working on auto doc
    Entered on [2020-02-26 mer. 17:32]
**** 2020-02-27 jeudi
***** worked on rtb transactions
    Entered on [2020-02-27 jeu. 13:50]
***** autodoc + rtb
    Entered on [2020-02-27 jeu. 17:28]
***** needs to check if rtb indexation works good
    Entered on [2020-02-27 jeu. 18:01]
**** 2020-02-28 vendredi
***** done jenkinsfile for data-scala-ml
    Entered on [2020-02-28 ven. 11:53]
*** 2020-03 mars
**** 2020-03-02 lundi
***** worked on curiosity : kafka consumer + producer test
    Entered on [2020-03-02 lun. 13:43]
**** 2020-03-03 mardi
***** done curiosity integration test
    Entered on [2020-03-03 mar. 11:54]
***** finished curiosity tests
    Entered on [2020-03-03 mar. 18:25]
***** needs to fix Get https://docker.ayl.io/v2/ayl/zookeeper/manifests/3.4.6: no basic auth credentials
    Entered on [2020-03-03 mar. 18:26]
**** 2020-03-10 mardi
***** currencies : done xml parsing + test
    Entered on [2020-03-10 mar. 14:29]
***** worked on nn job
    Entered on [2020-03-10 mar. 16:58]
**** 2020-03-12 jeudi
***** worked on sql alchemy
    Entered on [2020-03-12 jeu. 17:08]
***** modified integration test for curr
    Entered on [2020-03-12 jeu. 17:08]
**** 2020-03-13 vendredi
***** added platform backup to currnecy integration test
    Entered on [2020-03-13 ven. 15:33]
**** 2020-03-16 lundi
***** fixed git issue
    Entered on [2020-03-16 lun. 13:58]
***** done PR for currency
    Entered on [2020-03-16 lun. 13:59]
***** done data python utils PR
    Entered on [2020-03-16 lun. 15:37]
***** done db insertion for currencies
    Entered on [2020-03-16 lun. 17:07]
**** 2020-03-17 mardi
***** improved auto doc with formula and metric column + wrote doc
    Entered on [2020-03-17 mar. 15:45]
***** currencies PR fixes
    Entered on [2020-03-17 mar. 15:45]
***** currencies fixes
    Entered on [2020-03-17 mar. 15:56]
**** 2020-03-18 mercredi
***** continues working on doc
    Entered on [2020-03-18 mer. 16:33]
***** ask greg about how config python should be
    Entered on [2020-03-18 mer. 20:15]
**** 2020-03-19 jeudi
***** updated data etl doc PR
    Entered on [2020-03-19 jeu. 11:02]
***** started to work on preprod db for maxime
    Entered on [2020-03-19 jeu. 11:02]
***** worked on preprod db for billing stat
    Entered on [2020-03-19 jeu. 18:08]
**** 2020-03-20 vendredi
***** reviewed PRs
    Entered on [2020-03-20 ven. 15:18]
***** merged PR
    Entered on [2020-03-20 ven. 15:44]
**** 2020-03-23 lundi
***** put backup in docker run
took me all day...
    Entered on [2020-03-23 lun. 18:54]
***** data job scheduler PR for currencies
    Entered on [2020-03-23 lun. 19:38]
***** how to use secrets ?
**** 2020-03-24 mardi
***** fixed currency CI
    Entered on [2020-03-24 mar. 16:09]
***** merge data etl doc
    Entered on [2020-03-24 mar. 16:09]
***** worked on preprod db for billing
    Entered on [2020-03-24 mar. 16:28]
***** started to work on helm chart for currency db
    Entered on [2020-03-24 mar. 16:40]
**** 2020-03-25 mercredi
***** worked on creating db in kube
    Entered on [2020-03-25 mer. 16:44]
**** 2020-03-26 jeudi
***** spent a lot of time with kubernetes bc of postgresql pw issue
    Entered on [2020-03-26 jeu. 16:54]
**** 2020-03-27 vendredi
***** done platform backup
    Entered on [2020-03-27 ven. 15:37]
***** faced issue with jenkins basic auth creds
    Entered on [2020-03-27 ven. 17:19]
**** 2020-03-30 lundi
***** put currencies on hold bc of docker creds stuff
    Entered on [2020-03-30 lun. 16:17]
***** worked on open auction job
    Entered on [2020-03-30 lun. 16:18]
**** 2020-03-31 mardi
*****  done open auction PR
    Entered on [2020-03-31 mar. 20:01]
still needs to test the job
*** 2020-04 avril
**** 2020-04-01 mercredi
***** TODO change open auction job
    Entered on [2020-04-01 mer. 10:27]
add isOpenAuction column, and split job output on that column* Notes ADYL
***** finished open auction PR
    Entered on [2020-04-01 mer. 16:29]
***** pr fixes on preprod db billing stat
    Entered on [2020-04-01 mer. 16:49]
*** 2020-05 mai
**** 2020-05-04 lundi
***** worked on data python utils but still CI issues
    Entered on [2020-05-04 lun. 15:34]
***** made auto doc (upload confluence) PR
    Entered on [2020-05-04 lun. 20:39]
**** 2020-05-05 mardi
***** worked on data-python-utils and ssp-creatives-watcher CI
    Entered on [2020-05-05 mar. 15:45]
